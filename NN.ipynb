{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import joblib\n",
    "\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import SGD\n",
    "from keras.regularizers import l2\n",
    "\n",
    "sys.path.insert(0, '/Users/gokhan/libs')\n",
    "from Evio.FeatureGenerator import FeatureGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainDataFrame = pd.read_csv('./data/train.csv')\n",
    "\n",
    "# remove constant columns\n",
    "colsToRemove1 = []\n",
    "for col in trainDataFrame.columns:\n",
    "    if trainDataFrame[col].std() == 0:\n",
    "        colsToRemove1.append(col)\n",
    "\n",
    "trainDataFrame.drop(colsToRemove1, axis=1, inplace=True)\n",
    "\n",
    "# remove duplicate columns\n",
    "colsToRemove2 = []\n",
    "columns = trainDataFrame.columns\n",
    "for i in range(len(columns)-1):\n",
    "    v = trainDataFrame[columns[i]].values\n",
    "    for j in range(i+1,len(columns)):\n",
    "        if np.array_equal(v,trainDataFrame[columns[j]].values):\n",
    "            colsToRemove2.append(columns[j])\n",
    "\n",
    "trainDataFrame.drop(colsToRemove2, axis=1, inplace=True)\n",
    "\n",
    "trainLabels = trainDataFrame['TARGET']\n",
    "trainFeatures = trainDataFrame.drop(['ID','TARGET'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a homogenous non-skewed data set\n",
    "trainNonSkewed1 = trainDataFrame[trainDataFrame.TARGET == 1]\n",
    "trainNonSkewed0 = trainDataFrame[trainDataFrame.TARGET == 0][0:3500]\n",
    "trainNonSkewed = pd.concat([trainNonSkewed1, trainNonSkewed0])\n",
    "\n",
    "trainNonSkewedLabels = trainNonSkewed['TARGET']\n",
    "trainNonSkewedFeatures = trainNonSkewed.drop(['ID','TARGET'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainNonSkewed0_1 = trainDataFrame[trainDataFrame.TARGET == 0][0:3500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print trainDataFrame.shape\n",
    "print trainNonSkewed0_1.shape\n",
    "print trainNonSkewed1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = cross_validation.train_test_split(\n",
    "    trainNonSkewedFeatures, trainNonSkewedLabels, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scale data:\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# Don't cheat - fit only on training data\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled)\n",
    "\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_valid_scaled = pd.DataFrame(X_valid_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import network2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorized_result(j):\n",
    "    \"\"\"Return a 10-dimensional unit vector with a 1.0 in the jth\n",
    "    position and zeroes elsewhere.  This is used to convert a digit\n",
    "    (0...9) into a corresponding desired output from the neural\n",
    "    network.\"\"\"\n",
    "    e = np.zeros((2, 1))\n",
    "    e[j] = 1.0\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For scaled data:\n",
    "X_train_np = X_train_scaled.as_matrix()\n",
    "y_train_np = y_train.as_matrix()\n",
    "data_train = [(X_train_np[i,:], y_train_np[i]) for i in xrange(len(X_train_np))]\n",
    "data_train = [(np.reshape(x, (306, 1)), vectorized_result(y)) for x,y in data_train ]\n",
    "\n",
    "X_valid_np = X_valid_scaled.as_matrix()\n",
    "y_valid_np = y_valid.as_matrix()\n",
    "data_valid = [(X_valid_np[i,:], y_valid_np[i]) for i in xrange(len(X_valid_np))]\n",
    "data_valid = [(np.reshape(x, (306, 1)), y) for x,y in data_valid ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(data_train)\n",
    "print len(data_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net = network2.Network([306, 100, 100, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "e_cost, e_acc, tr_cost, tr_acc = net.SGD(data_train, epochs=250, mini_batch_size=10, eta=.01, lmbda=8.0, evaluation_data=data_valid,\n",
    "       monitor_evaluation_cost=True, monitor_evaluation_accuracy=True, monitor_training_cost=True, \n",
    "       monitor_training_accuracy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net.scores[\"val_acc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(e_cost, label='Validation Cost')\n",
    "\n",
    "plt.plot(tr_cost, label='Training Cost')\n",
    "\n",
    "#plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(np.array(e_acc)/1909.0, label='Validation Accuracy')\n",
    "plt.plot(np.array(tr_acc)/4452.0, label='Training Accuracy')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainNonSkewedLabels.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRY WITH 10,000 RANDOM FEATURES FROM RCE3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/Users/gokhan/libs\")\n",
    "from Evio.evolver import RandomConceptEvolver3\n",
    "\n",
    "rce3 = RandomConceptEvolver3(n_concepts=10000, n_gens=1, random_state=1, mate_selection=\"roulette\",\n",
    "                             eval_features_every_n_gens=0,\n",
    "                             eliminate_perc_features=0.0, eval_metric=\"auc\",\n",
    "                             n_attrs_per_feature=[(1, 0.05), (2, 0.25), (3, 0.25), (4, 0.25), (5, 0.2)])\n",
    "rce3.n_samples = trainNonSkewedFeatures.shape[0]\n",
    "#rce3.fit(trainNonSkewedFeatures, trainNonSkewedLabels,2,1)\n",
    "rce3._generate_features(trainNonSkewedFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_rce3, X_valid_rce3, y_train, y_valid = cross_validation.train_test_split(\n",
    "    rce3._features, trainNonSkewedLabels, test_size=0.3, random_state=1)\n",
    "\n",
    "# For scaled data:\n",
    "X_train_rce3_np = X_train_rce3\n",
    "#y_train_np = y_train.as_matrix()\n",
    "data_train_rce3 = [(X_train_rce3_np[i,:], y_train_np[i]) for i in xrange(len(X_train_rce3_np))]\n",
    "data_train_rce3 = [(np.reshape(x, (10000, 1)), vectorized_result(y)) for x,y in data_train_rce3 ]\n",
    "\n",
    "X_valid_rce3_np = X_valid_rce3\n",
    "#y_valid_np = y_valid.as_matrix()\n",
    "data_valid_rce3 = [(X_valid_rce3_np[i,:], y_valid_np[i]) for i in xrange(len(X_valid_rce3_np))]\n",
    "data_valid_rce3 = [(np.reshape(x, (10000, 1)), y) for x,y in data_valid_rce3 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(data_train_rce3)\n",
    "print len(data_valid_rce3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = network2.Network([10000, 100, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "e_cost, e_acc, tr_cost, tr_acc = net.SGD(data_train_rce3, epochs=250, mini_batch_size=10, eta=.0005, lmbda=5.0, \n",
    "                                            evaluation_data=data_valid_rce3, monitor_evaluation_cost=True, \n",
    "                                            monitor_evaluation_accuracy=True, monitor_training_cost=True, \n",
    "                                            monitor_training_accuracy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRY WITH BEST ~3900 BINARY FEATURES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainTransformedDF = pd.read_csv('./data/trainTransformed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainTransformedDF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataTarget0 = trainTransformedDF[trainTransformedDF.TARGET == 0]\n",
    "dataTarget1 = trainTransformedDF[trainTransformedDF.TARGET == 1]\n",
    "\n",
    "def getBalancedTrainAndValidationSets():\n",
    "    # shuffle\n",
    "    dataTarget0.reindex(np.random.permutation(dataTarget0.index))\n",
    "    dataTarget1.reindex(np.random.permutation(dataTarget1.index))\n",
    "\n",
    "    trn0 = dataTarget0[0:1500]\n",
    "    trn1 = dataTarget1[0:1500]\n",
    "    trn = pd.concat([trn0, trn1])\n",
    "    y_train = trn['TARGET']\n",
    "    X_train = trn.drop(['TARGET'], axis=1)\n",
    "    \n",
    "    val0 = dataTarget0[1500:3000]\n",
    "    val1 = dataTarget1[1500:]\n",
    "    val = pd.concat([val0, val1])\n",
    "    y_val = val['TARGET']\n",
    "    X_val = val.drop(['TARGET'], axis=1)\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_rce3, y_train, X_valid_rce3, y_valid = getBalancedTrainAndValidationSets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print X_train_rce3.shape\n",
    "print y_train.shape\n",
    "print X_valid_rce3.shape\n",
    "print y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# For scaled data:\n",
    "X_train_rce3_np = X_train_rce3.as_matrix()\n",
    "y_train_np = y_train.as_matrix()\n",
    "data_train_rce3 = [(X_train_rce3_np[i,:], y_train_np[i]) for i in xrange(len(X_train_rce3_np))]\n",
    "data_train_rce3 = [(np.reshape(x, (3920, 1)), vectorized_result(y)) for x,y in data_train_rce3 ]\n",
    "\n",
    "X_valid_rce3_np = X_valid_rce3.as_matrix()\n",
    "y_valid_np = y_valid.as_matrix()\n",
    "data_valid_rce3 = [(X_valid_rce3_np[i,:], y_valid_np[i]) for i in xrange(len(X_valid_rce3_np))]\n",
    "data_valid_rce3 = [(np.reshape(x, (3920, 1)), y) for x,y in data_valid_rce3 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(data_train_rce3)\n",
    "print len(data_valid_rce3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net = network2.Network([3920, 100, 2])\n",
    "e_cost, e_acc, tr_cost, tr_acc = net.SGD(data_train_rce3, epochs=50, mini_batch_size=10, eta=.01, lmbda=0.0, \n",
    "                                            evaluation_data=data_valid_rce3, monitor_evaluation_cost=True, \n",
    "                                            monitor_evaluation_accuracy=True, monitor_training_cost=True, \n",
    "                                            monitor_training_accuracy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KERAS cross-validated w best 3920 features or original scaled features\n",
    "for optimizers: http://keras.io/optimizers/#usage-of-optimizers\n",
    "for regularizers: http://keras.io/regularizers/#usage-of-regularizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorized_result(j):\n",
    "    \"\"\"Return a 10-dimensional unit vector with a 1.0 in the jth\n",
    "    position and zeroes elsewhere.  This is used to convert a digit\n",
    "    (0...9) into a corresponding desired output from the neural\n",
    "    network.\"\"\"\n",
    "    e = np.zeros((2, 1))\n",
    "    e[j] = 1.0\n",
    "    return e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BEST 3920 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainTransformedDF = pd.read_csv('./data/trainTransformed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainTransformedDF.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ORIGINAL SCALED FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainDataFrame = pd.read_csv('./data/train.csv')\n",
    "\n",
    "# remove constant columns\n",
    "colsToRemove1 = []\n",
    "for col in trainDataFrame.columns:\n",
    "    if trainDataFrame[col].std() == 0:\n",
    "        colsToRemove1.append(col)\n",
    "\n",
    "trainDataFrame.drop(colsToRemove1, axis=1, inplace=True)\n",
    "\n",
    "# remove duplicate columns\n",
    "colsToRemove2 = []\n",
    "columns = trainDataFrame.columns\n",
    "for i in range(len(columns)-1):\n",
    "    v = trainDataFrame[columns[i]].values\n",
    "    for j in range(i+1,len(columns)):\n",
    "        if np.array_equal(v,trainDataFrame[columns[j]].values):\n",
    "            colsToRemove2.append(columns[j])\n",
    "\n",
    "trainDataFrame.drop(colsToRemove2, axis=1, inplace=True)\n",
    "\n",
    "trainLabels = trainDataFrame['TARGET']\n",
    "#trainFeatures = trainDataFrame.drop(['ID','TARGET'], axis=1)\n",
    "trainTransformedDF = trainDataFrame.drop(['ID'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10,000 RANDOM FEATURES (not checked for uniqueness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainDataFrame = pd.read_csv('./data/train.csv')\n",
    "\n",
    "# remove constant columns\n",
    "colsToRemove1 = []\n",
    "for col in trainDataFrame.columns:\n",
    "    if trainDataFrame[col].std() == 0:\n",
    "        colsToRemove1.append(col)\n",
    "\n",
    "trainDataFrame.drop(colsToRemove1, axis=1, inplace=True)\n",
    "\n",
    "# remove duplicate columns\n",
    "colsToRemove2 = []\n",
    "columns = trainDataFrame.columns\n",
    "for i in range(len(columns)-1):\n",
    "    v = trainDataFrame[columns[i]].values\n",
    "    for j in range(i+1,len(columns)):\n",
    "        if np.array_equal(v,trainDataFrame[columns[j]].values):\n",
    "            colsToRemove2.append(columns[j])\n",
    "\n",
    "trainDataFrame.drop(colsToRemove2, axis=1, inplace=True)\n",
    "\n",
    "trainLabels = trainDataFrame['TARGET']\n",
    "trainFeatures = trainDataFrame.drop(['ID','TARGET'], axis=1)\n",
    "#trainTransformedDF = trainDataFrame.drop(['ID'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainDataFrame.shape: (76020, 308)\n",
      "trainLabels.shape: (76020,)\n",
      "trainFeatures.shape: (76020, 306)\n"
     ]
    }
   ],
   "source": [
    "print \"trainDataFrame.shape: %s\" % str(trainDataFrame.shape)\n",
    "print \"trainLabels.shape: %s\" % str(trainLabels.shape)\n",
    "print \"trainFeatures.shape: %s\" % str(trainFeatures.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 1.2 mins to generate 1000 features\n",
      "took total 1.3 mins to complete step 0\n",
      "took 2.6 mins to generate 1000 features\n",
      "took total 2.6 mins to complete step 1\n",
      "took 3.9 mins to generate 1000 features\n",
      "took total 4.0 mins to complete step 2\n",
      "took 5.3 mins to generate 1000 features\n",
      "took total 5.4 mins to complete step 3\n",
      "took 6.7 mins to generate 1000 features\n",
      "took total 6.9 mins to complete step 4\n",
      "took 8.2 mins to generate 1000 features\n",
      "took total 8.6 mins to complete step 5\n",
      "took 9.8 mins to generate 1000 features\n",
      "took total 10.3 mins to complete step 6\n",
      "took 11.5 mins to generate 1000 features\n",
      "took total 12.1 mins to complete step 7\n",
      "took 13.3 mins to generate 1000 features\n",
      "took total 14.0 mins to complete step 8\n",
      "took 15.3 mins to generate 1000 features\n",
      "took total 16.0 mins to complete step 9\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_features = 1000\n",
    "schema_10K = {}\n",
    "features_10K = pd.DataFrame(index=range(trainFeatures.shape[0]))\n",
    "for i in range(10):\n",
    "    start = time.time()\n",
    "    fg = FeatureGenerator(n_attrs_per_feature=[(1, 0.1), (2, 0.25), (3, 0.25), (4, 0.25), (5, 0.15)], random_state=1)\n",
    "    fg.last_feat_ix = i*n_features\n",
    "    schema, features = fg._generate_random_features(n_features, eval_score=False, X=trainFeatures)\n",
    "    print \"took %.1f mins to generate %d features\" % ((time.time()-start)/60., n_features)\n",
    "    schema_10K.update(schema)\n",
    "    features_10K = pd.concat([features_10K, features], axis=1)\n",
    "    print \"took total %.1f mins to complete step %d\" % ((time.time()-start)/60., i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features_10K.shape: (76020, 10000)\n",
      "len(schema_10K): 10000\n"
     ]
    }
   ],
   "source": [
    "print \"features_10K.shape: %s\" % str(features_10K.shape)\n",
    "print \"len(schema_10K): %d\" % len(schema_10K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainTransformedDF = pd.concat([features_10K, trainLabels], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainTransformedDF.shape: (76020, 10001)\n"
     ]
    }
   ],
   "source": [
    "print \"trainTransformedDF.shape: %s\" % str(trainTransformedDF.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### register cross validation function (with ability to scale data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataTarget0 = trainTransformedDF[trainTransformedDF.TARGET == 0]\n",
    "dataTarget1 = trainTransformedDF[trainTransformedDF.TARGET == 1]\n",
    "\n",
    "def getBalancedTrainAndValidationSets(scale=False):\n",
    "    # shuffle\n",
    "    dataTarget0.reindex(np.random.permutation(dataTarget0.index))\n",
    "    dataTarget1.reindex(np.random.permutation(dataTarget1.index))\n",
    "\n",
    "    trn0 = dataTarget0[0:1500]\n",
    "    trn1 = dataTarget1[0:1500]\n",
    "    trn = pd.concat([trn0, trn1])\n",
    "    y_train = trn['TARGET']\n",
    "    X_train = trn.drop(['TARGET'], axis=1)\n",
    "    \n",
    "    if scale:\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_train = pd.DataFrame(X_train)\n",
    "        \n",
    "    val0 = dataTarget0[1500:3000]\n",
    "    val1 = dataTarget1[1500:]\n",
    "    val = pd.concat([val0, val1])\n",
    "    y_val = val['TARGET']\n",
    "    X_val = val.drop(['TARGET'], axis=1)\n",
    "    \n",
    "    if scale:\n",
    "        X_val = scaler.transform(X_val)\n",
    "        X_val = pd.DataFrame(X_val)\n",
    "        \n",
    "    return X_train, y_train, X_val, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MANUAL EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save weights of the model! Beware, this saves ONLY weights. The model architecture must be reconstructed prior to load_weights()\n",
    "# to load weights: model.load_weights(weights_path)\n",
    "model_id = \"./models/keras_best_3920_50_2_sigm_softm_bin_crosent_sgd\"\n",
    "checkpointer = ModelCheckpoint(filepath=model_id+\".hdf5\", monitor='val_acc', verbose=1, save_best_only=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(output_dim=50, input_dim=3920))\n",
    "model.add(Activation('sigmoid')) # try also 'relu'\n",
    "model.add(Dense(output_dim=2))\n",
    "model.add(Activation(\"softmax\"))\n",
    "model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "config = model.get_config()\n",
    "joblib.dump(config, model_id+'.joblib')\n",
    "# to load config:\n",
    "# config = joblib.load(model_id+'.joblib')\n",
    "# to load (initiate) a saved model:\n",
    "# model = Sequential.from_config(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_rce3, y_train, X_valid_rce3, y_valid = getBalancedTrainAndValidationSets()\n",
    "y_train_vect = np.array([vectorized_result(x) for x in y_train])\n",
    "y_train_vect = np.reshape(y_train_vect, (y_train_vect.shape[0], 2))\n",
    "y_valid_vect = np.array([vectorized_result(x) for x in y_valid])\n",
    "y_valid_vect = np.reshape(y_valid_vect, (y_valid_vect.shape[0], 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hist = model.fit(X_train_rce3.as_matrix(), y_train_vect, verbose=1, batch_size=32, \n",
    "                 nb_epoch=5, validation_data=(X_valid_rce3.as_matrix(), y_valid_vect), callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(output_dim=50, input_dim=3920))\n",
    "model.add(Activation('sigmoid')) # try also 'relu'\n",
    "model.add(Dense(output_dim=2))\n",
    "model.add(Activation(\"softmax\"))\n",
    "model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "checkpointer = ModelCheckpoint(filepath=model_id+\".hdf5\", monitor='val_acc', verbose=1, save_best_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# to load (initiate) a saved model:\n",
    "config = joblib.load(model_id+'.joblib')\n",
    "model = Sequential.from_config(config)\n",
    "model.load_weights(model_id+'.hdf5')\n",
    "# compile params cannot be loaded from model config file: # Do NOT change them!!!\n",
    "model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "loss_and_metrics = model.evaluate(X_valid_rce3.as_matrix(), y_valid_vect, verbose=1)\n",
    "print loss_and_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.predict_proba(X_valid_rce3.as_matrix())[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "roc_auc_score(y_valid, model.predict_proba(X_valid_rce3.as_matrix())[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CROSS VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-359b63e95cfc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrn_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mval_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_folds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# set model_id and checkpointer object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "# check cross validated performance\n",
    "n_folds = 5\n",
    "scaled = False\n",
    "trn_scores = []\n",
    "val_scores = []\n",
    "start = time.time()\n",
    "for i in range(n_folds):\n",
    "    # set model_id and checkpointer object\n",
    "    model_id = \"./models/keras/last_keras_model_checkpoint\"\n",
    "    # to monitor validation accuracy:\n",
    "    checkpointer = ModelCheckpoint(filepath=model_id+\".hdf5\", monitor='val_loss', verbose=1, save_best_only=False)\n",
    "    # to monitor validation loss:\n",
    "    # checkpointer = ModelCheckpoint(filepath=model_id+\".hdf5\", verbose=1, save_best_only=True)\n",
    "    # get fold\n",
    "    X_train_rce3, y_train, X_valid_rce3, y_valid = getBalancedTrainAndValidationSets(scale=scaled)\n",
    "    y_train_vect = np.array([vectorized_result(x) for x in y_train])\n",
    "    y_train_vect = np.reshape(y_train_vect, (y_train_vect.shape[0], 2))\n",
    "    y_valid_vect = np.array([vectorized_result(x) for x in y_valid])\n",
    "    y_valid_vect = np.reshape(y_valid_vect, (y_valid_vect.shape[0], 2))\n",
    "    # create Sequential model\n",
    "    model = Sequential()\n",
    "    # 1. hidden layer\n",
    "    model.add(Dense(output_dim=100, input_dim=3920, W_regularizer=l2(0.01))) #W_regularizer=l2(0.1), \n",
    "    model.add(Activation('sigmoid'))\n",
    "    # 2. hidden\n",
    "    #model.add(Dense(output_dim=100, W_regularizer=l2(0.01))) #W_regularizer=l2(0.1), \n",
    "    #model.add(Activation('sigmoid'))\n",
    "    \n",
    "    #model.add(Dense(output_dim=100, W_regularizer=l2(0.01))) #W_regularizer=l2(0.1), \n",
    "    #model.add(Activation('sigmoid'))\n",
    "    \n",
    "    #model.add(Dense(output_dim=100, W_regularizer=l2(0.01))) #W_regularizer=l2(0.1), \n",
    "    #model.add(Activation('sigmoid'))\n",
    "    # output layer\n",
    "    model.add(Dense(output_dim=2, W_regularizer=l2(0.01))) #, W_regularizer=l2(0.1)\n",
    "    model.add(Activation(\"softmax\"))\n",
    "    # SGD(lr=0.005, momentum=0.1, nesterov=True)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "    model.fit(X_train_rce3.as_matrix(), y_train_vect, verbose=0, batch_size=10, \n",
    "                 nb_epoch=100, validation_data=(X_valid_rce3.as_matrix(), y_valid_vect), callbacks=[checkpointer])\n",
    "    \n",
    "    trn_score = roc_auc_score(y_train, model.predict_proba(X_train_rce3.as_matrix())[:,1])\n",
    "    val_score = roc_auc_score(y_valid, model.predict_proba(X_valid_rce3.as_matrix())[:,1])\n",
    "    print \"train auc: %.4f\" % trn_score\n",
    "    print \"validation auc: %.4f\" % val_score\n",
    "    trn_scores.append(trn_score)\n",
    "    val_scores.append(val_score)\n",
    "\n",
    "# save model architecture\n",
    "#config = model.get_config()\n",
    "#joblib.dump(config, model_id+'.joblib')\n",
    "\n",
    "print \"-----------------------\"\n",
    "print \"%d fold cross validation with %s data\" % (n_folds, \"SCALED\" if scaled else \"NON-SCALED\")\n",
    "print \"Train Mean: %.4f\" % np.mean(trn_scores)\n",
    "print \"Validation Mean: %.4f\" % np.mean(val_scores)\n",
    "print \"Total time (mins): %.1f\" % ((time.time()-start)/60.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATE SUBMISSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testDataFrame = pd.read_csv('./data/test.csv')\n",
    "\n",
    "# remove same columns as in training\n",
    "\n",
    "testDataFrame.drop(colsToRemove1, axis=1, inplace=True)\n",
    "testDataFrame.drop(colsToRemove1, axis=1, inplace=True)\n",
    "\n",
    "testIDs = trainDataFrame['ID']\n",
    "testFeatures = trainDataFrame.drop(['ID','TARGET'], axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
